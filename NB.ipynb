{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "129683c0",
   "metadata": {},
   "source": [
    "## Theoretical Introduction to Text Classification and NLP\n",
    "\n",
    "Text classification is one of the core tasks in Natural Language Processing (NLP).  \n",
    "In this laboratory, we focus on building a machine-learning model that automatically classifies text messages as **spam** or **ham** (not spam).  \n",
    "\n",
    "### What is Spam Classification?\n",
    "\n",
    "Spam detection is a binary classification problem in which the goal is to:\n",
    "- assign label **1** → spam,\n",
    "- assign label **0** → ham (legitimate message).\n",
    "\n",
    "It is widely used in email filtering, SMS moderation, and security systems.\n",
    "\n",
    "### NLP Preprocessing\n",
    "\n",
    "Raw text cannot be directly used by machine-learning models.  \n",
    "We must convert it into a numerical representation through several steps:\n",
    "\n",
    "1. **Cleaning and normalization**  \n",
    "   Lowercasing, removing punctuation, removing unnecessary characters.\n",
    "\n",
    "2. **Tokenization**  \n",
    "   Splitting text into individual words.\n",
    "\n",
    "3. **Stop-word removal**  \n",
    "   Removing very common words (e.g., *the*, *is*, *and*),  \n",
    "   which carry little information for classification.\n",
    "\n",
    "4. **Stemming**  \n",
    "   Reducing words to crude roots  \n",
    "   (*walking*, *walked* → *walk*).  \n",
    "   Fast but produces artificial word forms.\n",
    "\n",
    "5. **Lemmatization**  \n",
    "   Reducing words to dictionary forms  \n",
    "   (*better* → *good*, *cars* → *car*).  \n",
    "   More accurate but slower.\n",
    "\n",
    "### Vectorization\n",
    "\n",
    "Machine-learning models require numerical input, so text must be converted into vectors:\n",
    "\n",
    "- **Bag-of-Words (BoW)** — counts occurrences of each word.\n",
    "- **TF-IDF** — measures how important a word is in a document relative to the whole dataset.\n",
    "\n",
    "These methods transform text into a matrix that a classifier can use.\n",
    "\n",
    "### Machine-Learning Models\n",
    "\n",
    "For spam detection, common models include:\n",
    "- **Multinomial Naive Bayes** — fast baseline model.\n",
    "- **Logistic Regression** — strong linear classifier.\n",
    "- **Linear SVM** — often achieves high accuracy.\n",
    "- **Random Forest** — tree-based ensemble model.\n",
    "\n",
    "Each model has different strengths in handling sparse, high-dimensional text data.\n",
    "\n",
    "### Goal of the Laboratory\n",
    "\n",
    "During the exercises, you will:\n",
    "- preprocess and clean textual data,\n",
    "- compare stemming and lemmatization,\n",
    "- convert text into numerical features,\n",
    "- train multiple ML models,\n",
    "- evaluate and compare their performance,\n",
    "- analyze which words are most strongly associated with spam.\n",
    "\n",
    "This provides a complete workflow typical for real-world NLP classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5154c6bd",
   "metadata": {},
   "source": [
    "## Task 0 - Download data\n",
    "\n",
    "This code automatically downloads the **SMS Spam Collection** dataset from the UCI Machine Learning Repository.  \n",
    "It then extracts the ZIP file directly in memory, loads the raw text file into a pandas DataFrame, and finally saves it locally as `spam.csv`.\n",
    "\n",
    "This allows you to use a real, publicly available spam–ham dataset without manually downloading or preparing any files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89b691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Pobierz ZIP z UCI\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "r = requests.get(url)\n",
    "\n",
    "# Rozpakowanie z pamięci\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()\n",
    "\n",
    "# Odczyt pliku SMSSpamCollection\n",
    "df = pd.read_csv(\n",
    "    \"SMSSpamCollection\",\n",
    "    sep=\"\\t\",\n",
    "    names=[\"v1\", \"v2\"],\n",
    "    encoding=\"latin-1\"\n",
    ")\n",
    "\n",
    "# Zapis CSV w formacie jak Kaggle\n",
    "df.to_csv(\"spam.csv\", index=False, encoding=\"latin-1\")\n",
    "\n",
    "print(\"Utworzono spam.csv z UCI SMS Spam Collection!\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58163aa1",
   "metadata": {},
   "source": [
    "## Task 1 — Build a Basic SPAM/HAM Classifier\n",
    "\n",
    "In this task, you will create a simple machine-learning model that classifies text messages as **spam** or **ham** (not spam).  \n",
    "You will work with the SMS Spam Collection dataset from UCI.\n",
    "\n",
    "### What you need to do:\n",
    "\n",
    "1. **Load the dataset** (`spam.csv`) and keep two columns:\n",
    "   - `label` (spam/ham)\n",
    "   - `text` (message content)\n",
    "\n",
    "2. **Convert labels**:\n",
    "   - `ham` → `0`\n",
    "   - `spam` → `1`\n",
    "\n",
    "3. **Explore the dataset**:\n",
    "   - Display the first few rows\n",
    "   - Show how many spam and ham messages are in the dataset\n",
    "\n",
    "4. **Split data** into training and test sets (80/20).\n",
    "   Use `stratify=y` to preserve class distribution.\n",
    "\n",
    "5. **Vectorize the text** using Bag-of-Words (`CountVectorizer`).\n",
    "\n",
    "6. **Train a Multinomial Naive Bayes model** on the vectorized data.\n",
    "\n",
    "7. **Evaluate the model** using:\n",
    "   - Accuracy  \n",
    "   - Precision (spam)  \n",
    "   - Recall (spam)  \n",
    "   - F1-score (spam)  \n",
    "   - Confusion matrix  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d4c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1 – Basic SPAM / HAM Classifier (Template)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# 1. Load the dataset.\n",
    "# The file \"spam.csv\" must be located in the working directory.\n",
    "# The dataset should contain at least two columns: one with labels and one with message text.\n",
    "df = pd.read_csv(\"spam.csv\", encoding=\"latin-1\")\n",
    "\n",
    "# Select the relevant columns and rename them.\n",
    "# Typically, the dataset contains columns such as \"v1\" (label) and \"v2\" (text),\n",
    "# along with several unused columns.\n",
    "df = df[['v1', 'v2']]\n",
    "df.columns = ['label', 'text']\n",
    "\n",
    "# 2. Convert categorical labels into numerical form.\n",
    "# The standard mapping is: \"ham\" → 0, \"spam\" → 1.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Display a short preview and class distribution.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4. Define features and target variables.\n",
    "# X should contain text messages, and y should contain the numerical labels.\n",
    "\n",
    "\n",
    "# Split the dataset into training and test subsets (80/20),\n",
    "# using stratified sampling to preserve label proportions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5. Vectorize the text using the Bag-of-Words method.\n",
    "# CountVectorizer transforms text into a sparse numerical matrix.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 6. Train a Multinomial Naive Bayes classifier.\n",
    "# This model is commonly used for text classification tasks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 7. Evaluate the classifier.\n",
    "# Compute accuracy, precision (spam), recall (spam), F1-score (spam),\n",
    "# and display the confusion matrix for detailed error analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a414bc56",
   "metadata": {},
   "source": [
    "## Task 2 — Compare Text Preprocessing Techniques: No Cleaning vs. Stemming vs. Lemmatization\n",
    "\n",
    "In this task, you will investigate how different text-cleaning approaches influence the performance of a machine-learning model.\n",
    "\n",
    "You will compare **three versions** of the SMS dataset:\n",
    "\n",
    "1. **No stemming or lemmatization**  \n",
    "2. **With stemming** (PorterStemmer)  \n",
    "3. **With lemmatization** (WordNetLemmatizer)\n",
    "\n",
    "### Steps to complete:\n",
    "\n",
    "1. Implement a `clean_text()` function that:\n",
    "   - converts text to lowercase,  \n",
    "   - removes punctuation and digits,  \n",
    "   - tokenizes text,  \n",
    "   - removes stopwords,  \n",
    "   - optionally applies:\n",
    "     - stemming (`mode=\"stem\"`),\n",
    "     - lemmatization (`mode=\"lemma\"`),\n",
    "     - or nothing (`mode=\"none\"`).\n",
    "\n",
    "2. Create 3 new columns in your DataFrame:\n",
    "   - `text_clean_none`\n",
    "   - `text_clean_stem`\n",
    "   - `text_clean_lemma`\n",
    "\n",
    "3. For each version of cleaned text:\n",
    "   - split into train/test sets,\n",
    "   - vectorize using **TF-IDF** (`TfidfVectorizer`),\n",
    "   - train a **Logistic Regression** classifier,\n",
    "   - compute precision, recall, and F1-score for the *spam* class.\n",
    "\n",
    "4. Create a comparison table with:\n",
    "   - accuracy,\n",
    "   - precision (spam),\n",
    "   - recall (spam),\n",
    "   - F1-score (spam).\n",
    "\n",
    "5. (optional) Consider following questions:\n",
    "   - Which cleaning method performs best?\n",
    "   - Does stemming produce unnatural word roots?\n",
    "   - Does lemmatization create more interpretable features?\n",
    "   - Does more preprocessing always mean better performance?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a125a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for first use\n",
    " \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f515fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text, mode=\"none\"):\n",
    "    \"\"\"\n",
    "    Clean and normalize text, optionally applying stemming or lemmatization.\n",
    "    mode: \"none\" / \"stem\" / \"lemma\"\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "# Assume that the DataFrame `df` from Task 1 is already available\n",
    "# and contains the columns: 'text' (raw messages) and 'label_num' (numeric labels).\n",
    "\n",
    "# Apply text cleaning for all preprocessing variants\n",
    "# Generate three cleaned text columns:\n",
    "#  - text_clean_none  (no stemming/lemmatization)\n",
    "#  - text_clean_stem  (stemming)\n",
    "#  - text_clean_lemma (lemmatization)\n",
    "# Each should be created using df['text'].apply(clean_text, ...).\n",
    "\n",
    "\n",
    "\n",
    "###Model Training and Evaluation for Each Preprocessing Variant\n",
    "# For each version of the cleaned text (no preprocessing, stemming, lemmatization), the following procedure must be applied:\n",
    "\n",
    "# 1. Split the data into training and test sets using stratified sampling to preserve label distribution.  \n",
    "# 2. Convert the textual data into numerical features using TF-IDF vectorization (`TfidfVectorizer`).  \n",
    "# 3. Train a Logistic Regression classifier on the vectorized training data.  \n",
    "# 4. Evaluate the model by computing precision, recall, and F1-score specifically for the *spam* class, as it is typically the minority class and harder to detect.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Performance Comparison Table\n",
    "# After evaluating all three preprocessing variants, construct a comparison table that includes the following metrics for each variant:\n",
    "\n",
    "# - **Accuracy** — overall proportion of correct predictions.  \n",
    "# - **Precision (spam)** — proportion of messages predicted as spam that are actually spam.  \n",
    "# - **Recall (spam)** — proportion of true spam messages correctly detected by the model.  \n",
    "# - **F1-score (spam)** — harmonic mean of precision and recall, providing a balanced measure of classifier performance.\n",
    "\n",
    "# This table will allow a direct comparison of how preprocessing choices influence the effectiveness of the spam-detection model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815029a9",
   "metadata": {},
   "source": [
    "## Task 3 — Compare ML Models and Identify the Most “Spammy” Words\n",
    "\n",
    "In this task, you will:\n",
    "1. Train multiple machine-learning models,\n",
    "2. Compare their performance,\n",
    "3. Identify the most important words that indicate spam.\n",
    "\n",
    "### Steps to complete:\n",
    "\n",
    "1. Choose the best-performing text version from Task 2  \n",
    "   (usually `text_clean_lemma`).  \n",
    "\n",
    "2. Vectorize the text using **TF-IDF** with a limited vocabulary size  \n",
    "   (e.g., `max_features=3000`).\n",
    "\n",
    "3. Train and evaluate the following ML models:\n",
    "   - **Multinomial Naive Bayes**\n",
    "   - **Logistic Regression**\n",
    "   - **Linear SVM (LinearSVC)**\n",
    "   - **Random Forest Classifier**\n",
    "\n",
    "4. For each model, compute:\n",
    "   - Accuracy  \n",
    "   - Precision (spam)  \n",
    "   - Recall (spam)  \n",
    "   - F1-score (spam)\n",
    "\n",
    "5. Create a summary table comparing all models.\n",
    "\n",
    "6. Select the best *linear* model  \n",
    "   (Logistic Regression or LinearSVC).\n",
    "\n",
    "7. Extract and display the top **20 most spam-indicative words**, based on:\n",
    "   - highest positive coefficients → words strongly associated with spam  \n",
    "   - lowest negative coefficients → words strongly associated with ham  \n",
    "\n",
    "8. Plot horizontal bar charts showing the top 10 spam and ham words.\n",
    "\n",
    "9. **Interpretation**:\n",
    "   - What kinds of words typically appear in spam?  \n",
    "   - What words are common in ham (normal) messages?  \n",
    "   - Which ML model performed best, and why do you think so?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b79961",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
